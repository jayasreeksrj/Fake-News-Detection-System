{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6926829268292682\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.67      0.96      0.79       249\n",
      "        Real       0.83      0.27      0.41       161\n",
      "\n",
      "    accuracy                           0.69       410\n",
      "   macro avg       0.75      0.62      0.60       410\n",
      "weighted avg       0.73      0.69      0.64       410\n",
      "\n",
      "Model and vectorizer saved as 'fake_news_detection_model.pkl' and 'tfidf_vectorizer.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "\n",
    "# Load dataset (replace 'news_articles.csv' with the path to your dataset)\n",
    "data = pd.read_csv('news_articles.csv')\n",
    "\n",
    "# Data preprocessing\n",
    "# We'll use the 'text' and 'label' columns for this task\n",
    "data = data[['text', 'label']].dropna()\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text transformation using TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Model training - Logistic Regression\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Saving the trained model and TF-IDF vectorizer for future use\n",
    "joblib.dump(model, 'fake_news_detection_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n",
    "\n",
    "print(\"Model and vectorizer saved as 'fake_news_detection_model.pkl' and 'tfidf_vectorizer.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ismatsamadov/Fake_News_Detection/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2024-10-13 11:00:54,117] A new study created in memory with name: no-name-96dc690d-6a60-4d78-9d4e-b2453758b43a\n",
      "[I 2024-10-13 11:00:55,783] Trial 0 finished with value: 0.6359753389003846 and parameters: {'C': 0.0017670169402947942, 'max_df': 0.9753571532049581}. Best is trial 0 with value: 0.6359753389003846.\n",
      "[I 2024-10-13 11:00:56,357] Trial 1 finished with value: 0.644512302647586 and parameters: {'C': 0.24658329458549083, 'max_df': 0.7993292420985183}. Best is trial 1 with value: 0.644512302647586.\n",
      "[I 2024-10-13 11:00:56,926] Trial 2 finished with value: 0.6359753389003846 and parameters: {'C': 8.632008168602535e-05, 'max_df': 0.5779972601681014}. Best is trial 1 with value: 0.644512302647586.\n",
      "[I 2024-10-13 11:00:57,491] Trial 3 finished with value: 0.6359753389003846 and parameters: {'C': 2.231010801867923e-05, 'max_df': 0.9330880728874675}. Best is trial 1 with value: 0.644512302647586.\n",
      "[I 2024-10-13 11:00:58,066] Trial 4 finished with value: 0.6359753389003846 and parameters: {'C': 0.040428727350273294, 'max_df': 0.8540362888980227}. Best is trial 1 with value: 0.644512302647586.\n",
      "[I 2024-10-13 11:00:58,319] Trial 5 finished with value: 0.6359753389003846 and parameters: {'C': 1.3289448722869181e-05, 'max_df': 0.9849549260809971}. Best is trial 1 with value: 0.644512302647586.\n",
      "[I 2024-10-13 11:00:58,598] Trial 6 finished with value: 0.6890241588596252 and parameters: {'C': 0.9877700294007905, 'max_df': 0.6061695553391381}. Best is trial 6 with value: 0.6890241588596252.\n",
      "[I 2024-10-13 11:00:58,833] Trial 7 finished with value: 0.6359753389003846 and parameters: {'C': 0.00012329623163659834, 'max_df': 0.5917022549267169}. Best is trial 6 with value: 0.6890241588596252.\n",
      "[I 2024-10-13 11:00:59,069] Trial 8 finished with value: 0.6359753389003846 and parameters: {'C': 0.0006690421166498799, 'max_df': 0.762378215816119}. Best is trial 6 with value: 0.6890241588596252.\n",
      "[I 2024-10-13 11:00:59,307] Trial 9 finished with value: 0.6359753389003846 and parameters: {'C': 0.0039054412752107894, 'max_df': 0.645614570099021}. Best is trial 6 with value: 0.6890241588596252.\n",
      "[I 2024-10-13 11:00:59,616] Trial 10 finished with value: 0.7414658264749673 and parameters: {'C': 7.37864208342294, 'max_df': 0.5029985914779085}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:00:59,943] Trial 11 finished with value: 0.7408553258644667 and parameters: {'C': 6.564817611753482, 'max_df': 0.5026485604069959}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:00,264] Trial 12 finished with value: 0.7402437091650986 and parameters: {'C': 8.822079130997952, 'max_df': 0.5193625999805915}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:00,588] Trial 13 finished with value: 0.7402448252539661 and parameters: {'C': 8.110348201977573, 'max_df': 0.505446330602451}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:00,863] Trial 14 finished with value: 0.6713397307547216 and parameters: {'C': 0.6420689367136895, 'max_df': 0.6898021217101118}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:01,113] Trial 15 finished with value: 0.6359753389003846 and parameters: {'C': 0.09484191925606972, 'max_df': 0.68711186412056}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:01,407] Trial 16 finished with value: 0.709762206105899 and parameters: {'C': 1.709280466591459, 'max_df': 0.5459020293597581}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:01,661] Trial 17 finished with value: 0.6359753389003846 and parameters: {'C': 0.04120639428703559, 'max_df': 0.659143125062404}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:01,961] Trial 18 finished with value: 0.725616248468168 and parameters: {'C': 2.6453113157569486, 'max_df': 0.502086166494805}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:02,208] Trial 19 finished with value: 0.6359753389003846 and parameters: {'C': 0.012759667849030359, 'max_df': 0.5720456338607967}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:02,480] Trial 20 finished with value: 0.6469520729118535 and parameters: {'C': 0.3066660423448515, 'max_df': 0.8447683005603212}. Best is trial 10 with value: 0.7414658264749673.\n",
      "[I 2024-10-13 11:01:02,791] Trial 21 finished with value: 0.7420729788188655 and parameters: {'C': 9.297431007889415, 'max_df': 0.5097754659842255}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:03,107] Trial 22 finished with value: 0.7347581323815328 and parameters: {'C': 4.076777418708559, 'max_df': 0.5487460066805564}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:03,423] Trial 23 finished with value: 0.7408542097755992 and parameters: {'C': 9.50762315930149, 'max_df': 0.6111748579258214}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:03,719] Trial 24 finished with value: 0.7073224358416316 and parameters: {'C': 1.6097672336838413, 'max_df': 0.541411040386185}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:03,992] Trial 25 finished with value: 0.644512302647586 and parameters: {'C': 0.24292254020119322, 'max_df': 0.630480540239765}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:04,295] Trial 26 finished with value: 0.7329310949055007 and parameters: {'C': 3.682800825795157, 'max_df': 0.5630107742790804}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:04,571] Trial 27 finished with value: 0.6719502313652223 and parameters: {'C': 0.7202338190884896, 'max_df': 0.7131201303740432}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:04,819] Trial 28 finished with value: 0.6359753389003846 and parameters: {'C': 0.09976325479158793, 'max_df': 0.5062529812977768}. Best is trial 21 with value: 0.7420729788188655.\n",
      "[I 2024-10-13 11:01:05,118] Trial 29 finished with value: 0.7323194782061327 and parameters: {'C': 3.7687183443794536, 'max_df': 0.9264332835059756}. Best is trial 21 with value: 0.7420729788188655.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'C': 9.297431007889415, 'max_df': 0.5097754659842255}\n",
      "Accuracy: 0.7634146341463415\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Fake       0.76      0.89      0.82       249\n",
      "        Real       0.77      0.57      0.65       161\n",
      "\n",
      "    accuracy                           0.76       410\n",
      "   macro avg       0.77      0.73      0.74       410\n",
      "weighted avg       0.76      0.76      0.75       410\n",
      "\n",
      "                                                   Text Predicted Label  \\\n",
      "1808  home  be the change  government corruption  pr...            Fake   \n",
      "694   podcast play in new window  download  embed \\n...            Fake   \n",
      "906   new leaked clinton emails came from the device...            Fake   \n",
      "544   email \\n\\nhillary supporter robert dougherty f...            Fake   \n",
      "1847  id love to see clinton spend all her money and...            Fake   \n",
      "\n",
      "     Actual Label  \n",
      "1808         Real  \n",
      "694          Fake  \n",
      "906          Fake  \n",
      "544          Fake  \n",
      "1847         Fake  \n",
      "Model and vectorizer saved as 'optimized_fake_news_detection_model.pkl' and 'optimized_tfidf_vectorizer.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import joblib\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('news_articles.csv')\n",
    "\n",
    "# Data preprocessing - select the 'text' and 'label' columns, drop any missing values\n",
    "data = data[['text', 'label']].dropna()\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model optimization process using Optuna\n",
    "def objective(trial):\n",
    "    # Hyperparameters for tuning\n",
    "    C = trial.suggest_float(\"C\", 1e-5, 10.0, log=True)\n",
    "    max_df = trial.suggest_float(\"max_df\", 0.5, 1.0)\n",
    "    stop_words = 'english'\n",
    "    \n",
    "    # Create the pipeline: TF-IDF and Logistic Regression\n",
    "    model_pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer(stop_words=stop_words, max_df=max_df)),\n",
    "        ('log_reg', LogisticRegression(C=C, max_iter=1000, random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Perform cross-validation\n",
    "    score = cross_val_score(model_pipeline, X_train, y_train, n_jobs=-1, cv=3, scoring=\"accuracy\")\n",
    "    return score.mean()\n",
    "\n",
    "# Create Optuna study\n",
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "study.optimize(objective, n_trials=30)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "# Build the final model using the best hyperparameters\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=best_params['max_df'])\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train the Logistic Regression model with optimized hyperparameters\n",
    "model = LogisticRegression(C=best_params['C'], max_iter=1000, random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:')\n",
    "print(report)\n",
    "\n",
    "# Save the model and vectorizer\n",
    "joblib.dump(model, 'optimized_fake_news_detection_model.pkl')\n",
    "joblib.dump(tfidf_vectorizer, 'optimized_tfidf_vectorizer.pkl')\n",
    "\n",
    "# Testing function to predict new examples\n",
    "def test_model(examples):\n",
    "    examples_tfidf = tfidf_vectorizer.transform(examples)\n",
    "    predictions = model.predict(examples_tfidf)\n",
    "    return predictions\n",
    "\n",
    "# Example test over the dataset\n",
    "test_examples = X_test[:5]\n",
    "predicted_labels = test_model(test_examples)\n",
    "\n",
    "# Display test results\n",
    "test_results = pd.DataFrame({\n",
    "    'Text': test_examples,\n",
    "    'Predicted Label': predicted_labels,\n",
    "    'Actual Label': y_test[:5].values\n",
    "})\n",
    "\n",
    "print(test_results)\n",
    "\n",
    "# Save the test results\n",
    "test_results.to_csv('test_results.csv', index=False)\n",
    "\n",
    "print(\"Model and vectorizer saved as 'optimized_fake_news_detection_model.pkl' and 'optimized_tfidf_vectorizer.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
